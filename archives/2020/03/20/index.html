<!DOCTYPE html>
<html>
  <head>
  <title>
      
           - Logan
      
  </title>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="马荣贺" />
  <link rel="shortcut icon" type="image/x-icon" href="https://maronghe.github.io/img/">

  
  
  
  
  
  <link rel="stylesheet" href="https://maronghe.github.io/style.min.a50e85be2191b7446e684667eb002377b10f833f329c30d9b701ea188a664750.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">

  
  

  <meta property="og:title" content="" />
<meta property="og:description" content="DIstributed Cron Job In Micor
定时任务几乎在所有的项目后台存在，可能在特定是时间执行一次或周期性的执行。
 单机
 ​	在单机中的服务中，一般定时任务是直截了当和简单的。
但是在多节点中的定时任务，通常可能有===任务重复执行和单点故障问题===。
1.一些’伙计‘将定时任务从守护进程中移到Web服务中并暴露API或RPC接口，然后触发器通过操作系统执行任务。或通过负载均衡，去请求一个节点。但是还是有单点故障问题。
但是还是能同时解决 重复执行 和 单点故障问题。这正是面临的经典问题 ： Leader选举。
Micor 可以创建定时任务
// get etcd node list from registry  etcdList := service.Options().Registry.Options().Addrs // build leader  lead := etcd.NewLeader(leader.Nodes(etcdList...)) cron := sync.NewCron(sync.WithLeader(lead)) cron.Schedule( task.Schedule{Interval: 10 * time.Second}, task.Command{Name: &#34;foo&#34;, Func: func() error { log.Info(&#34;finish command foo&#34;) return nil }}, ) Note： 其依赖 Etcd、zookeeper等分布式选举中间件。其实之前自己实现的通过Redis实现的分布式锁的逻辑，也是一个自我周期性获取Heath Check 和 抢占获取锁（选举当Leader）的过程。
task.Schedule =&gt; 就两个内容 -&gt; // Schedule represents a time or interval at which a task should run type Schedule struct { // When to start the schedule." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://maronghe.github.io/archives/2020/03/20/" />

<meta itemprop="name" content="">
<meta itemprop="description" content="DIstributed Cron Job In Micor
定时任务几乎在所有的项目后台存在，可能在特定是时间执行一次或周期性的执行。
 单机
 ​	在单机中的服务中，一般定时任务是直截了当和简单的。
但是在多节点中的定时任务，通常可能有===任务重复执行和单点故障问题===。
1.一些’伙计‘将定时任务从守护进程中移到Web服务中并暴露API或RPC接口，然后触发器通过操作系统执行任务。或通过负载均衡，去请求一个节点。但是还是有单点故障问题。
但是还是能同时解决 重复执行 和 单点故障问题。这正是面临的经典问题 ： Leader选举。
Micor 可以创建定时任务
// get etcd node list from registry  etcdList := service.Options().Registry.Options().Addrs // build leader  lead := etcd.NewLeader(leader.Nodes(etcdList...)) cron := sync.NewCron(sync.WithLeader(lead)) cron.Schedule( task.Schedule{Interval: 10 * time.Second}, task.Command{Name: &#34;foo&#34;, Func: func() error { log.Info(&#34;finish command foo&#34;) return nil }}, ) Note： 其依赖 Etcd、zookeeper等分布式选举中间件。其实之前自己实现的通过Redis实现的分布式锁的逻辑，也是一个自我周期性获取Heath Check 和 抢占获取锁（选举当Leader）的过程。
task.Schedule =&gt; 就两个内容 -&gt; // Schedule represents a time or interval at which a task should run type Schedule struct { // When to start the schedule.">

<meta itemprop="wordCount" content="954">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:image" content="https://maronghe.github.io//img/"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="DIstributed Cron Job In Micor
定时任务几乎在所有的项目后台存在，可能在特定是时间执行一次或周期性的执行。
 单机
 ​	在单机中的服务中，一般定时任务是直截了当和简单的。
但是在多节点中的定时任务，通常可能有===任务重复执行和单点故障问题===。
1.一些’伙计‘将定时任务从守护进程中移到Web服务中并暴露API或RPC接口，然后触发器通过操作系统执行任务。或通过负载均衡，去请求一个节点。但是还是有单点故障问题。
但是还是能同时解决 重复执行 和 单点故障问题。这正是面临的经典问题 ： Leader选举。
Micor 可以创建定时任务
// get etcd node list from registry  etcdList := service.Options().Registry.Options().Addrs // build leader  lead := etcd.NewLeader(leader.Nodes(etcdList...)) cron := sync.NewCron(sync.WithLeader(lead)) cron.Schedule( task.Schedule{Interval: 10 * time.Second}, task.Command{Name: &#34;foo&#34;, Func: func() error { log.Info(&#34;finish command foo&#34;) return nil }}, ) Note： 其依赖 Etcd、zookeeper等分布式选举中间件。其实之前自己实现的通过Redis实现的分布式锁的逻辑，也是一个自我周期性获取Heath Check 和 抢占获取锁（选举当Leader）的过程。
task.Schedule =&gt; 就两个内容 -&gt; // Schedule represents a time or interval at which a task should run type Schedule struct { // When to start the schedule."/>

  <!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
  <![endif]-->

  <!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
  <![endif]-->

  </head>

  <body>
    
  <h1></h1>
  <header>
  
  <div class="avatar">
    <img class="avatarMask" src="https://maronghe.github.io//img/">
    <a href="https://maronghe.github.io/"><img src="https://maronghe.github.io//img/avatar-border.svg"></a>
  </div>
  
  <h2><a class="author" href="https://maronghe.github.io/">马荣贺</a></h2>
</header>

  
  
  <p class="date">January 1, 0001</p>
  
  <div id="tags">
    <ul>
      
    </ul>
  </div>
  
  <div id="contentBody">
    <p>DIstributed Cron Job In Micor</p>
<p>定时任务几乎在所有的项目后台存在，可能在特定是时间执行一次或周期性的执行。</p>
<blockquote>
<p>单机</p>
</blockquote>
<p>​	在单机中的服务中，一般定时任务是直截了当和简单的。</p>
<p>但是在多节点中的定时任务，通常可能有===任务重复执行和单点故障问题===。</p>
<p>1.一些’伙计‘将定时任务从守护进程中移到Web服务中并暴露API或RPC接口，然后触发器通过操作系统执行任务。或通过负载均衡，去请求一个节点。但是还是有单点故障问题。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gepuuebpi1j30jc0b1js5.jpg" alt=""></p>
<p>但是还是能同时解决 重复执行 和 单点故障问题。这正是面临的经典问题 ： Leader选举。</p>
<p>Micor 可以创建定时任务</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go">  <span style="color:#75715e">// get etcd node list from registry
</span><span style="color:#75715e"></span>   <span style="color:#a6e22e">etcdList</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">service</span>.<span style="color:#a6e22e">Options</span>().<span style="color:#a6e22e">Registry</span>.<span style="color:#a6e22e">Options</span>().<span style="color:#a6e22e">Addrs</span>
   <span style="color:#75715e">// build leader
</span><span style="color:#75715e"></span>   <span style="color:#a6e22e">lead</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">etcd</span>.<span style="color:#a6e22e">NewLeader</span>(<span style="color:#a6e22e">leader</span>.<span style="color:#a6e22e">Nodes</span>(<span style="color:#a6e22e">etcdList</span><span style="color:#f92672">...</span>))
   
   <span style="color:#a6e22e">cron</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">sync</span>.<span style="color:#a6e22e">NewCron</span>(<span style="color:#a6e22e">sync</span>.<span style="color:#a6e22e">WithLeader</span>(<span style="color:#a6e22e">lead</span>))
   <span style="color:#a6e22e">cron</span>.<span style="color:#a6e22e">Schedule</span>(
      <span style="color:#a6e22e">task</span>.<span style="color:#a6e22e">Schedule</span>{<span style="color:#a6e22e">Interval</span>: <span style="color:#ae81ff">10</span> <span style="color:#f92672">*</span> <span style="color:#a6e22e">time</span>.<span style="color:#a6e22e">Second</span>},
      <span style="color:#a6e22e">task</span>.<span style="color:#a6e22e">Command</span>{<span style="color:#a6e22e">Name</span>: <span style="color:#e6db74">&#34;foo&#34;</span>, <span style="color:#a6e22e">Func</span>: <span style="color:#66d9ef">func</span>() <span style="color:#66d9ef">error</span> {
         <span style="color:#a6e22e">log</span>.<span style="color:#a6e22e">Info</span>(<span style="color:#e6db74">&#34;finish command foo&#34;</span>)
         <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">nil</span>
      }},
   )
</code></pre></div><p>Note： 其依赖 Etcd、zookeeper等分布式选举中间件。其实之前自己实现的通过Redis实现的分布式锁的逻辑，也是一个自我周期性获取Heath Check 和 抢占获取锁（选举当Leader）的过程。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#a6e22e">task</span>.<span style="color:#a6e22e">Schedule</span> =&gt; <span style="color:#a6e22e">就两个内容</span> <span style="color:#f92672">-</span>&gt; 

<span style="color:#75715e">// Schedule represents a time or interval at which a task should run
</span><span style="color:#75715e"></span><span style="color:#66d9ef">type</span> <span style="color:#a6e22e">Schedule</span> <span style="color:#66d9ef">struct</span> {
   <span style="color:#75715e">// When to start the schedule. Zero time means immediately
</span><span style="color:#75715e"></span>   <span style="color:#a6e22e">Time</span> <span style="color:#a6e22e">time</span>.<span style="color:#a6e22e">Time</span>
   <span style="color:#75715e">// Non zero interval dictates an ongoing schedule
</span><span style="color:#75715e"></span>   <span style="color:#a6e22e">Interval</span> <span style="color:#a6e22e">time</span>.<span style="color:#a6e22e">Duration</span>
}

</code></pre></div><blockquote>
<p>服务熔断和限流</p>
</blockquote>
<p><a href="https://github.com/micro/go-plugins/tree/master/wrapper/breaker/hystrix">hystrix</a> &amp; <a href="https://github.com/micro/go-plugins/tree/master/wrapper/breaker/gobreaker">gobreaker</a></p>
<p>插件帮我们解决：</p>
<p><strong>DefaultMaxConcurrent</strong> 是对于三个服务还还是3个不同的方法呢？</p>
<p><a href="https://github.com/micro/go-plugins/blob/master/wrapper/breaker/hystrix/hystrix.go">https://github.com/micro/go-plugins/blob/master/wrapper/breaker/hystrix/hystrix.go</a></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#66d9ef">func</span> (<span style="color:#a6e22e">c</span> <span style="color:#f92672">*</span><span style="color:#a6e22e">clientWrapper</span>) <span style="color:#a6e22e">Call</span>(<span style="color:#a6e22e">ctx</span> <span style="color:#a6e22e">context</span>.<span style="color:#a6e22e">Context</span>, <span style="color:#a6e22e">req</span> <span style="color:#a6e22e">client</span>.<span style="color:#a6e22e">Request</span>, <span style="color:#a6e22e">rsp</span> <span style="color:#66d9ef">interface</span>{}, <span style="color:#a6e22e">opts</span> <span style="color:#f92672">...</span><span style="color:#a6e22e">client</span>.<span style="color:#a6e22e">CallOption</span>) <span style="color:#66d9ef">error</span> {
	<span style="color:#66d9ef">return</span> <span style="color:#a6e22e">hystrix</span>.<span style="color:#a6e22e">Do</span>(<span style="color:#a6e22e">req</span>.<span style="color:#a6e22e">Service</span>()<span style="color:#f92672">+</span><span style="color:#e6db74">&#34;.&#34;</span><span style="color:#f92672">+</span><span style="color:#a6e22e">req</span>.<span style="color:#a6e22e">Endpoint</span>(), <span style="color:#66d9ef">func</span>() <span style="color:#66d9ef">error</span> {
		<span style="color:#66d9ef">return</span> <span style="color:#a6e22e">c</span>.<span style="color:#a6e22e">Client</span>.<span style="color:#a6e22e">Call</span>(<span style="color:#a6e22e">ctx</span>, <span style="color:#a6e22e">req</span>, <span style="color:#a6e22e">rsp</span>, <span style="color:#a6e22e">opts</span><span style="color:#f92672">...</span>)
	}, <span style="color:#66d9ef">nil</span>)
}


<span style="color:#f92672">...</span>
<span style="color:#a6e22e">hystrix</span>.<span style="color:#a6e22e">ConfigureCommand</span>(<span style="color:#e6db74">&#34;com.serviceA.methodFoo&#34;</span>,
   <span style="color:#a6e22e">hystrix</span>.<span style="color:#a6e22e">CommandConfig</span>{
      <span style="color:#a6e22e">MaxConcurrentRequests</span>: <span style="color:#ae81ff">50</span>, <span style="color:#75715e">// 最大并发请求数 每个服务每个节点的每个方法
</span><span style="color:#75715e"></span>      <span style="color:#a6e22e">Timeout</span>:               <span style="color:#ae81ff">10</span>, <span style="color:#75715e">// 超时时间
</span><span style="color:#75715e"></span>   })
<span style="color:#a6e22e">hystrix</span>.<span style="color:#a6e22e">ConfigureCommand</span>(<span style="color:#e6db74">&#34;com.serviceB.methodBar&#34;</span>,
   <span style="color:#a6e22e">hystrix</span>.<span style="color:#a6e22e">CommandConfig</span>{
      <span style="color:#a6e22e">Timeout</span>: <span style="color:#ae81ff">60</span>,
   })
<span style="color:#f92672">...</span>

<span style="color:#75715e">// req.Service()+&#34;.&#34;+req.Endpoint() 意味着服务和端点，并没有管节点的数量，
</span><span style="color:#75715e">// so : each method of each service counts independently and does not affect each other.
</span><span style="color:#75715e">// 每个服务的每个方法数量的限制，并不影响其他的。
</span><span style="color:#75715e"></span>
</code></pre></div><p><a href="https://github.com/micro/go-plugins/tree/master/wrapper/ratelimiter/uber">Rate Limiter</a></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go"><span style="color:#f92672">package</span> <span style="color:#a6e22e">main</span>
<span style="color:#f92672">import</span> (
   <span style="color:#a6e22e">limiter</span> <span style="color:#e6db74">&#34;github.com/micro/go-plugins/wrapper/ratelimiter/uber/v2&#34;</span>
)
<span style="color:#66d9ef">func</span> <span style="color:#a6e22e">main</span>() {
   <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">QPS</span> = <span style="color:#ae81ff">100</span>
   <span style="color:#75715e">// New Service
</span><span style="color:#75715e"></span>   <span style="color:#a6e22e">service</span> <span style="color:#f92672">:=</span> <span style="color:#a6e22e">micro</span>.<span style="color:#a6e22e">NewService</span>(
      <span style="color:#a6e22e">micro</span>.<span style="color:#a6e22e">Name</span>(<span style="color:#e6db74">&#34;com.foo.srv.hello&#34;</span>),
      <span style="color:#a6e22e">micro</span>.<span style="color:#a6e22e">Version</span>(<span style="color:#e6db74">&#34;latest&#34;</span>),
      <span style="color:#a6e22e">micro</span>.<span style="color:#a6e22e">WrapHandler</span>(<span style="color:#a6e22e">limiter</span>.<span style="color:#a6e22e">NewHandlerWrapper</span>(<span style="color:#a6e22e">QPS</span>)), <span style="color:#75715e">// 限制接口最大并发数
</span><span style="color:#75715e"></span>   )
}
</code></pre></div><p>Final Worlds :</p>
<p>The role of Circuit Breaker is to protect the client from being dragged down by external service issues and always respond quickly (even if you get an error, it is better than waiting for a long time). Always avoid excessive consumption of resources (避免过度消耗资源).</p>
<p>While the role of Rate Limiter is to protect the server. Only handle the traffic within its capacity to achieve overload protection. An error is returned immediately when the traffic exceeds the preset limit.</p>
<blockquote>
<p>ZooKeeper  （借鉴于文件系统）</p>
</blockquote>
<ol>
<li>Zookeeper 用于 分布式 系统中协调任务。</li>
<li>任务可以是协作的（主给从分配任务）、也可以是竞争关系（竞争主节点争取执行权，实现互斥排他锁）。</li>
<li>特性：
<ol>
<li>保证CAP中的CP（即强一致性，持久性（分区容错和顺序性））</li>
<li><!-- raw HTML omitted -->实现通用的同步原语能力<!-- raw HTML omitted -->？？？</li>
<li>提供简单的并发处理机制</li>
</ol>
</li>
</ol>
<p>Zookeeper :</p>
<ol>
<li>setup /conf/zoo.cfg file. include port、dataDir、tickTime and so on.</li>
<li><code>bin/zkServer.sh start </code> start a server.</li>
<li><code>bin/zkCli.sh -server 127.0.0.1:2181</code> connect the local zk server.</li>
<li>&hellip; ls / , create /my_zk hello , set /my_zk world , delete /my_zk &hellip; For more <a href="https://zookeeper.apache.org/doc/current/zookeeperCLI.html">detail</a>.</li>
<li>强烈建议zk集群节点为奇数，否则无法完成节点选取。（大于一半以上同意才能当成leader）</li>
<li>Finally, note the two port numbers after each server name: &quot; 2888&rdquo; and &ldquo;3888&rdquo;. Peers use the former port to connect to other peers. Such a connection is necessary so that peers can communicate, for example, to agree upon the order of updates. More specifically, a ZooKeeper server uses this port to connect followers to the leader. When a new leader arises, a follower opens a TCP connection to the leader using this port. Because the default leader election also uses TCP, we currently require another port for leader election. This is the second port in the server entry.</li>
<li>最后我们注意到有每个server都有两个端口号，<strong>端口对等使用</strong>。若需要保证一个必要的连接，以至于对等方可以通信。例如同意更新顺序，更特别的是，一个zk sever使用一个端口连接followers到leader。当一个新leader成立之后，一个从节点打开一个TCP连接到leader使用这个端口。因为leader的选举也是用过TCP，我们要求leader选举另外一个端口号，这就是第二个端口号在server entry中。</li>
<li>总结一句话， 从节点一个端口用于连接TCP连接与主节点，当阶段选举时，也要用过端口使用TCP进行选举。</li>
</ol>
<p>分布式锁中重要的特性之一是，如果节点获取锁后当<code>Carsh</code>后必须释放锁。</p>
<p>zookeeper 可以用过 <code>create -e /lock 's2:2228'</code>创建临时节点去加锁，正是利用了当session断开后，连接自动被删除的特性实现。同时，可以通过<code>watch</code>命令进行查看状态并监听<code>state -w /lock</code>，如果监听的节点断开后，会通知到监听节点一个<code>event</code>节点被删除的事件。</p>
<ol>
<li>
<p>myid file is missing：</p>
<p>需要在dataDir下创建myid文件并且需要在里面指定当前版本<code>1</code></p>
<p>zk 分布式锁，</p>
<p>使用临时顺序节点</p>
<p>创建最小后缀数字znode的用户获取到锁。</p>
<p>避免羊群效应。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gerxfrtkfcj30ih07uwfj.jpg" alt=""></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gerxfztgjij311g0di0xg.jpg" alt=""></p>
<p>watch前一个节点的状态，此锁还能遵循公平原则。先到的锁请求先处理。仅通知一个锁的等待者。避免通知所有。</p>
<p>2 <code>watch</code> 1 , 3 <code>watch</code> 2</p>
</li>
</ol>
<p><code>./bin/zkCli.sh -server 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183</code></p>
<p>随机选取一个节点连接。</p>
<p>如果节点A（非住检点）宕机后，则客户端会收到一个事件(Event ， Closing Socket Exception)，则会从其他节点中选取一个进行连接。</p>
<p>如果一个主节点宕机之后从节点由于连接不上主节点之后会在仲裁模（Quorum）式下进行leader选举</p>
<pre><code>2020-05-14 17:05:20,780 [myid:3] - INFO  [QuorumPeer[myid=3](plain=[0:0:0:0:0:0:0:0]:2183)(secure=disabled):ZooKeeperServer@329] - Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 clientPortListenBacklog -1 datadir /Users/logan/Documents/env/zookeeper3/data/version-2 snapdir /Users/logan/Documents/env/zookeeper3/data/version-2
2020-05-14 17:05:20,782 [myid:3] - INFO  [QuorumPeer[myid=3](plain=[0:0:0:0:0:0:0:0]:2183)(secure=disabled):Leader@581] - LEADING - LEADER ELECTION TOOK - 221 MS
2020-05-14 17:05:20,783 [myid:3] - INFO  [QuorumPeer[myid=3](plain=[0:0:0:0:0:0:0:0]:2183)(secure=disabled):QuorumPeer@863] - Peer state changed: leading - discovery
2020-05-14 17:05:20,784 [myid:3] - INFO  [QuorumPeer[myid=3](plain=[0:0:0:0:0:0:0:0]:2183)(secure=disabled):FileTxnSnapLog@470] - Snapshotting: 0x900000007 to /Users/logan/Documents/env/zookeeper3/data/version-2/snapshot.900000007
2020-05-14 17:05:20,784 [myid:3] - INFO  [QuorumPeer[myid=3](plain=[0:0:0:0:0:0:0:0]:2183)(secure=disabled):ZooKeeperServer@519] - Snapshot taken in 1 ms
2020-05-14 17:05:20,889 [myid:3] - INFO  [LearnerHandler-/127.0.0.1:59479:LearnerHandler@504] - Follower sid: 1 : info : 127.0.0.1:2777:3777:participant
2020-05-14 17:05:20,891 [myid:3] - INFO  [LearnerHandler-/127.0.0.1:59479:ZKDatabase@345] - On disk txn sync enabled with snapshotSizeFactor 0.33
2020-05-14 17:05:20,892 [myid:3] - INFO  [LearnerHandler-/127.0.0.1:59479:LearnerHandler@800] - Synchronizing with Learner sid: 1 maxCommittedLog=0x900000007 minCommittedLog=0x900000001 lastProcessedZxid=0x900000007 peerLastZxid=0x900000007
2020-05-14 17:05:20,892 [myid:3] - INFO  [LearnerHandler-/127.0.0.1:59479:LearnerHandler@845] - Sending DIFF zxid=0x900000007 for peer sid: 1
2020-05-14 17:05:20,892 [myid:3] - INFO  [QuorumPeer[myid=3](plain=[0:0:0:0:0:0:0:0]:2183)(secure=disabled):QuorumPeer@863] - Peer state changed: leading - synchronization
2020-05-14 17:05:20,897 [myid:3] - INFO  [QuorumPeer[myid=3](plain=[0:0:0:0:0:0:0:0]:2183)(secure=disabled):Leader@1501] - Have quorum of supporters, sids: [[1, 3],[1, 3]]; starting up and setting last processed zxid: 0xa00000000
2020-05-14 17:05:20,898 [myid:3] - INFO  [QuorumPeer[myid=3](plain=[0:0:0:0:0:0:0:0]:2183)(secure=disabled):CommitProcessor@476] - Configuring CommitProcessor with readBatchSize -1 commitBatchSize 1
2020-05-14 17:05:20,899 [myid:3] - INFO  [QuorumPeer[myid=3](plain=[0:0:0:0:0:0:0:0]:2183)(secure=disabled):CommitProcessor@438] - Configuring CommitProcessor with 12 worker threads.
2020-05-14 17:05:20,906 [myid:3] - INFO  [QuorumPeer[myid=3](plain=[0:0:0:0:0:0:0:0]:2183)(secure=disabled):ContainerManager@83] - Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0
2020-05-14 17:05:20,908 [myid:3] - INFO  [QuorumPeer[myid=3](plain=[0:0:0:0:0:0:0:0]:2183)(secure=disabled):QuorumPeer@863] - Peer state changed: leading - broadcast
</code></pre><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ges4rnks3qj31l20p6wsm.jpg" alt=""></p>
<p><code>dataTree 保存在内容中 8G It fine.</code></p>
<p>CPU不是性能的瓶颈，所以双核CPU就可以。存储设备的写延迟会直接影响事务的提交效率。所以建议分配一个独立的dataLogDir的SSD磁盘。</p>
<p>// zk监控 JMS + Prometheus</p>
<p>// Observe 实现 zk 跨区域部署，优化节点响应速度。 发送 forward 的等待 infor</p>
<p>节点间的 propose、ack、commit消息跨区域</p>
<p>动态配置：<a href="https://zookeeper.apache.org/doc/current/zookeeperReconfig.html#sc_reconfig_retrieving">dynamic configuration</a></p>
<pre><code>服务发现
服务实例获取
服务反馈（Curator）
</code></pre><pre><code>etcd 
k8s 服务发现 和 配置中心
OpenStack 配置中心 和 分布式锁
ROOK 编排引擎

zk datatree 需要加载到内存 几百兆
etcd bbolt 存储引擎 几个GB  k-v 存储

MVCC 
compaction reversion 进行整理和清除之前的版本
bbolt -&gt; b+ tree  key : major + sub + type 
	1.major -&gt; reversion
	2.sub -&gt; 每次更新的key
	3.type 保存可选项的特殊值 （11 22 平台）
内存中还维护了一个btree，是key-value 中的 key


</code></pre><pre><code>LSM Log Struct Merge-Tree
B+ Tree
And Compare with B+ Tre and LSM 
</code></pre><p>// 1 简历与要求要高度匹配</p>
<p>// 2 体现自己能干活</p>
<p>// 3 体现自己非常愿意以正式员工并长久的留在大厂工作</p>
<blockquote>
<p>TODO</p>
</blockquote>
<ol>
<li>基于Kubernetes的CICD的环境部署实施</li>
<li></li>
</ol>
<p>Slice扩容规则</p>
<p>1.预估扩容后的容量 newCap -&gt; 预估元素个数</p>
<p>​	如果 扩容前容量翻倍还是小于最小长度，那么就等于最小长度 oldCap * 2 &lt; cap， <strong>newCap = cap</strong></p>
<p>​	否则 如果扩容前长度(oldCap)小于1024 则 <strong>newCap = oldCap * 2</strong></p>
<p>​			 否则  &gt;= 1024 则 扩容1/4 <strong>newCap = oldCap * 1.25</strong></p>
<p>2.newCap个元素需多大内存？（与元素类型大小挂钩）</p>
<p>​	newCap * 元素大小 = 所需内存大小（并非直接向操作系统申请内存，与各语言内存管理模块有关）</p>
<ol start="3">
<li>匹配合适的内存规格。Go内管理模块分为：8 ，16，32，48，64，80，96，112&hellip;.</li>
</ol>
<p>​	如 我要申请24字节内存，则 Go直接分配 32 字节内存。（匹配足够大且最接近的规格）</p>
<pre><code>
	s := []int{1, 2} // int 8 字节
	s = append(s, 3, 4, 5)

	// step 1  3 * 2 = 6
	// 5 &gt; 4 = 5
	// 5 * 8 = 40 &lt;= 48
	// 48 / 8 = 6
	fmt.Println(len(s))   // 5
	fmt.Println(cap(s))   // 6
	fmt.Printf(&quot;%p\n&quot;, s) // 0xc00001c180
	s = append(s, 6)
	fmt.Printf(&quot;%p\n&quot;, s) // 0xc00001c180

	fmt.Println(len(s)) // 6
	fmt.Println(cap(s)) // 6
</code></pre><pre><code>a := []string{&quot;my&quot;,&quot;name&quot;,&quot;is&quot;} // 64位 16字节
a = append(a,&quot;Logan&quot;)
// 3 * 2 &gt; 4 , newCap = 6
// 16 * 6 = 96 字节
// newCap = 16


</code></pre><p>迁移不是从cell 0位置开始迁移的</p>

  </div>
  <footer>
  <p>
  &copy;  马荣贺.
  </p>
</footer>


  </body>
</html>
